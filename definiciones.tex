\chapter{Definiciones preliminares} % Contexto de la tesis (background formal, y contexto del desarrollo
En este capítulo se revisarán algunas definiciones de conceptos técnicos, para posteriormente utilizarlos sin ambiguedad durante el resto de la presentación.

\section{Agente inteligente}
Un agente es una entidad computacional autónoma, que puede percibir su entorno a través de sensores, y actuar en dicho entorno utilizando efectores. Usualmente, la información que un agente percibe de su entorno es sólo parcial. Los agentes toman decisiones a partir de la información contenida en su base de conocimiento, siguiendo diferentes conjuntos de reglas propuestas, y actúan de manera acorde a la decisión tomada. Dichas acciones, a su vez, pueden producir efectos en el entorno.

Actualmente los agentes tienen un campo de aplicación muy amplio y existen
muchos tipos de agentes diferentes (por ejemplo: \textit{reactivos}, \textit{deliberativos},
\textit{inteligentes}, \textit{de interface}, \textit{colaborativos}), los cuales a su vez están
orientados a distintos entornos de aplicación.

En la mayoría de los casos, los agentes no existen por sí solos, sino que participan de un Sistema Multi-Agente (SMA).

%El objetivo particular de este proyecto es la aplicación de Argumentación para
%la implementación de diálogos entre agentes inmersos en un escenario con
%objetivos determinados. Puntualmente, se enfocará la investigación a la
%plataforma propuesta en el Multi-Agent Programming Contest, un juego académico
%donde agentes independientes compiten por diferentes objetivos.\\
%Sin embargo, el desarrollo de herramientas para implementar tales formalismos se encuentra en
%progreso y a un paso más lento. Además, muchas de las herramientas disponibles
%carecen de una base formal y suelen ser simplemente un entorno de desarrollo
%amigable.\\


\section{Sistema Multi-Agente}
Es un sistema en el cual muchos agentes interactúan para conseguir algún objetivo o realizar alguna tarea común. En los
sistemas multi-agente, cada agente tiene información incompleta y capacidades limitadas, el control del sistema es distribuido,
los datos están descentralizados, y la computación es asincrónica. Los agentes se desenvuelven en un entorno dinámico y
cambiante, el cual no puede predecirse y se ve afectado por las acciones que son llevadas a cabo.

Un aspecto importante en SMA es la comunicación entre agentes, la cual puede
ser necesaria para que los agentes compitan o cooperen de acuerdo a sus metas
individuales. Los diálogos con otros agentes del mismo ambiente son, actualmente, un área de estudio intensivo.


\input{preliminares_bdi.tex}

\input{preliminares_delp.tex}

\section{Multi-Agent Programming Contest}
El \textit{Multi-Agent Programming Contest} es un concurso de programación de
Inteligencia Artificial iniciado en el año 2005 con el objetivo de estimular la
investigación en el área de desarrollo y programación de Sistemas Multi-Agente.
Para ello, la competencia propone diferentes escenarios de juego de manera
anual, que obligan a los participantes tanto a identificar y resolver problemas
clave, como a explorar lenguajes, plataformas y herramientas de programación
para Sistemas Multi-Agente.

\subsection{Escenario MAPC 2011}

El escenario del año 2011 está formado por el mapa de un planeta representado
mediante un grafo. Cada nodo del grafo es una locación válida (y tiene un valor
determinado), y existen arcos (con diferente costo de energía) que permiten a un
agente desplazarse de una locación a otra.

En cada ronda de la competición participan dos equipos rivales. Cada equipo
posee un conjunto de agentes con diferentes roles preestablecidos
(\textit{Explorador}, \textit{Saboteador}, \textit{Reparador},
\textit{Sentinela} e \textit{Inspector}). El rol de cada agente define tanto el
conjunto de acciones que puede realizar, como sus características físicas
(\textit{Energía}, \textit{Salud}, \textit{Fuerza} y \textit{Rango de
Visión}).

\subsubsection{Puntaje}
La simulación del juego se desarrolla por pasos, y en cada paso se otorga a los
equipos una determinada cantidad de puntos según el estado de la simulación. El
objetivo del juego es obtener la mayor cantidad de puntos posibles cuando la
simulación termina.

Para obtener puntos, los agentes de cada uno de los equipos deben lograr formar
\textit{"`zonas"'} en el mapa logrando posicionarse en diferentes locaciones de
manera estratégica. La predominancia de un equipo sobre el otro en los nodos es
determinada por un algoritmo bien definido para la competencia, y el valor de
todos los nodos dominados por un equipo es el principal factor del puntaje
otorgado en cada uno de los pasos de la simulación. Algunas otras situaciones,
como el logro de determinados \textit{achievements}, pueden otorgar puntos
adicionales y dinero al equipo.
\subsubsection{Acciones}
Todos los agentes tienen acciones en común que pueden realizar en cada uno de
los pasos de la simulación:
\begin{itemize}
	\item goto(X): el agente se desplaza hacia el nodo X, siempre y cuando
exista un arco que conecte el nodo actual del agente con X, y dicho arco tenga
un costo menor a la energía actual del agente.
	\item survey(X): el agente recibe en su próxima percepción los costos de
todos los arcos conectados al nodo en el que se encuentra actualmente.
	\item buy(X): el agente utiliza el dinero obtenido a partir de los
\textit{achievements} para aumentar el valor máximo de cualquiera de sus
características físicas (Energía, Salud, Fuerza o Rango de visión) en 1 punto.
	\item recharge: el agente recupera el 20\% de su energía máxima.
	\item skip: el agente pasa al turno siguiente sin realizar ningún tipo
de acción.
\end{itemize}

Además, según el rol de cada agente, existen algunas acciones específicas que
pueden realizar:
\begin{itemize}
	\item attack(X): acción disponible únicamente para los \textit{Saboteadores}; el
agente ataca a un enemigo X, si dicho enemigo se encuentra en el mismo nodo. El
ataque, de tener éxito, decrementa la energía del agente enemigo, pudiendo
deshabilitarlo en caso de que ésta llegue a 0.
	\item parry: acción disponible únicamente para los \textit{Reparadores},
\textit{Saboteadores} y \textit{Sentinelas}. La acción protege al agente de los ataques enemigos,
impidiendo que éstos tengan éxito.
	\item probe: acción disponible únicamente para los \textit{Exploradores}. El
agente recibe en su próxima percepción el valor del nodo en el que se encuentra
actualmente. Ésta acción no sólo resulta importante por conocer el valor del
nodo, sino que además permite que, cuando el nodo es conquistado por el equipo,
dicho valor se sume al total de puntos de la zona. Un nodo en el que no se
realizó \textit{probe} suma únicamente 1 punto al valor total de la zona.
	\item inspect: acción disponible únicamente para los \textit{Inspectores}. El
inspector recibe en su próxima percepción la información física (Salud, Energía,
Fuerza, Rango de visión) de todos los agentes enemigos que se encuentren en el
mismo nodo que él, o en cualquier vecino directo.
	\item repair(X): acción disponible únicamente para los \textit{Reparadores}. El
reparador aumenta el valor de la Salud actual de su compañero de equipo X
(volviendo a habilitarlo, en caso de que su Salud fuera 0).

\end{itemize}

\subsubsection{Toma de decisiones y motivación para la resolución de conflictos}
Dado que cada agente decide por separado qué acción tomar, muchas veces ocurre que dos (o más) de los agentes del mismo
equipo realizan acciones que resultan redundantes, peligrosas, y en el peor de
los casos, perjudiciales al combinarse. Como mencionamos en el ejemplo introductorio de la tesis, en el caso de que dos agentes realicen una acción idéntica a la vez, existe la posibilidad de que dicha planificación represente un malgasto de tiempo o recursos para los agentes; en el marco de MAPC 2011, esto ocurre, por ejemplo, cuando dos agentes cualesquiera tienen como intención survey($X_{i}$), cuando dos exploradores tienen como intención probear($X_{i}$), o cuando dos inspectores tienen como intención inspect estando en el mismo nodo.
Si bien por limitaciones temporales durante la competencia únicamente se realizaron coordinaciones implícitas en las acciones de los agentes, es naturalmente posible mejorar dicha coordinación para sacar mayor rédito de las acciones, y ésta es la principal motivación de este trabajo.